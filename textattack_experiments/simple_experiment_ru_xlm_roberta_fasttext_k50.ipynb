{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "varied-bobby",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "- This IS expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from ru_test import dataset, labels\n",
    "from ru_xlm_roberta_model import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "atomic-carroll",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_to_id = {label: label_id for label_id, label in enumerate(labels)}\n",
    "id_to_label = {label_id: label for label_id, label in enumerate(labels)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "surface-australia",
   "metadata": {},
   "outputs": [],
   "source": [
    "from textattack.constraints.grammaticality import PartOfSpeech\n",
    "from textattack.constraints.pre_transformation import (\n",
    "    InputColumnModification,\n",
    "    RepeatModification,\n",
    "    StopwordModification,\n",
    ")\n",
    "from textattack.constraints.semantics import WordEmbeddingDistance\n",
    "from textattack.constraints.semantics.sentence_encoders import UniversalSentenceEncoder\n",
    "\n",
    "from textattack.attack_recipes import TextFoolerJin2019\n",
    "from textattack.goal_functions import UntargetedClassification\n",
    "from textattack.search_methods import GreedyWordSwapWIR\n",
    "from textattack.transformations import WordSwapEmbedding\n",
    "from textattack.shared.attack import Attack\n",
    "\n",
    "from textattack.shared import WordEmbedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "robust-vision",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "embedding_matrix = np.load('ru_fasttext_30000/embeddings_matrix.npy')\n",
    "nn_matrix = np.load('ru_fasttext_30000/nn_matrix.npy')\n",
    "index2word = pickle.load(open('ru_fasttext_30000/index2word.pcl', 'rb'))\n",
    "word2index = pickle.load(open('ru_fasttext_30000/word2index.pcl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "detailed-issue",
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "\n",
    "smart_word2index = collections.defaultdict(int, word2index)\n",
    "smart_index2word = collections.defaultdict(str, index2word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "black-genealogy",
   "metadata": {},
   "outputs": [],
   "source": [
    "ru_fasttext_embedding = WordEmbedding(embedding_matrix, word2index, index2word, nn_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "divine-provincial",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_textfooler(model):\n",
    "    #\n",
    "    # Swap words with their 50 closest embedding nearest-neighbors.\n",
    "    # Embedding: Counter-fitted PARAGRAM-SL999 vectors.\n",
    "    #\n",
    "    transformation = WordSwapEmbedding(max_candidates=50, embedding=ru_fasttext_embedding)\n",
    "    #\n",
    "    # Don't modify the same word twice or the stopwords defined\n",
    "    # in the TextFooler public implementation.\n",
    "    #\n",
    "    # fmt: off\n",
    "    stopwords = set(\n",
    "    #    [\"a\", \"about\", \"above\", \"across\", \"after\", \"afterwards\", \"again\", \"against\", \"ain\", \"all\", \"almost\", \"alone\", \"along\", \"already\", \"also\", \"although\", \"am\", \"among\", \"amongst\", \"an\", \"and\", \"another\", \"any\", \"anyhow\", \"anyone\", \"anything\", \"anyway\", \"anywhere\", \"are\", \"aren\", \"aren't\", \"around\", \"as\", \"at\", \"back\", \"been\", \"before\", \"beforehand\", \"behind\", \"being\", \"below\", \"beside\", \"besides\", \"between\", \"beyond\", \"both\", \"but\", \"by\", \"can\", \"cannot\", \"could\", \"couldn\", \"couldn't\", \"d\", \"didn\", \"didn't\", \"doesn\", \"doesn't\", \"don\", \"don't\", \"down\", \"due\", \"during\", \"either\", \"else\", \"elsewhere\", \"empty\", \"enough\", \"even\", \"ever\", \"everyone\", \"everything\", \"everywhere\", \"except\", \"first\", \"for\", \"former\", \"formerly\", \"from\", \"hadn\", \"hadn't\", \"hasn\", \"hasn't\", \"haven\", \"haven't\", \"he\", \"hence\", \"her\", \"here\", \"hereafter\", \"hereby\", \"herein\", \"hereupon\", \"hers\", \"herself\", \"him\", \"himself\", \"his\", \"how\", \"however\", \"hundred\", \"i\", \"if\", \"in\", \"indeed\", \"into\", \"is\", \"isn\", \"isn't\", \"it\", \"it's\", \"its\", \"itself\", \"just\", \"latter\", \"latterly\", \"least\", \"ll\", \"may\", \"me\", \"meanwhile\", \"mightn\", \"mightn't\", \"mine\", \"more\", \"moreover\", \"most\", \"mostly\", \"must\", \"mustn\", \"mustn't\", \"my\", \"myself\", \"namely\", \"needn\", \"needn't\", \"neither\", \"never\", \"nevertheless\", \"next\", \"no\", \"nobody\", \"none\", \"noone\", \"nor\", \"not\", \"nothing\", \"now\", \"nowhere\", \"o\", \"of\", \"off\", \"on\", \"once\", \"one\", \"only\", \"onto\", \"or\", \"other\", \"others\", \"otherwise\", \"our\", \"ours\", \"ourselves\", \"out\", \"over\", \"per\", \"please\", \"s\", \"same\", \"shan\", \"shan't\", \"she\", \"she's\", \"should've\", \"shouldn\", \"shouldn't\", \"somehow\", \"something\", \"sometime\", \"somewhere\", \"such\", \"t\", \"than\", \"that\", \"that'll\", \"the\", \"their\", \"theirs\", \"them\", \"themselves\", \"then\", \"thence\", \"there\", \"thereafter\", \"thereby\", \"therefore\", \"therein\", \"thereupon\", \"these\", \"they\", \"this\", \"those\", \"through\", \"throughout\", \"thru\", \"thus\", \"to\", \"too\", \"toward\", \"towards\", \"under\", \"unless\", \"until\", \"up\", \"upon\", \"used\", \"ve\", \"was\", \"wasn\", \"wasn't\", \"we\", \"were\", \"weren\", \"weren't\", \"what\", \"whatever\", \"when\", \"whence\", \"whenever\", \"where\", \"whereafter\", \"whereas\", \"whereby\", \"wherein\", \"whereupon\", \"wherever\", \"whether\", \"which\", \"while\", \"whither\", \"who\", \"whoever\", \"whole\", \"whom\", \"whose\", \"why\", \"with\", \"within\", \"without\", \"won\", \"won't\", \"would\", \"wouldn\", \"wouldn't\", \"y\", \"yet\", \"you\", \"you'd\", \"you'll\", \"you're\", \"you've\", \"your\", \"yours\", \"yourself\", \"yourselves\"]\n",
    "    )\n",
    "    # fmt: on\n",
    "    constraints = [RepeatModification(), StopwordModification(stopwords=stopwords)]\n",
    "    #\n",
    "    # During entailment, we should only edit the hypothesis - keep the premise\n",
    "    # the same.\n",
    "    #\n",
    "    input_column_modification = InputColumnModification(\n",
    "        [\"premise\", \"hypothesis\"], {\"premise\"}\n",
    "    )\n",
    "    constraints.append(input_column_modification)\n",
    "    # Minimum word embedding cosine similarity of 0.5.\n",
    "    # (The paper claims 0.7, but analysis of the released code and some empirical\n",
    "    # results show that it's 0.5.)\n",
    "    #\n",
    "    \n",
    "    constraints.append(WordEmbeddingDistance(min_cos_sim=0.5))\n",
    "    \n",
    "    #\n",
    "    # Only replace words with the same part of speech (or nouns with verbs)\n",
    "    #\n",
    "    \n",
    "    #constraints.append(PartOfSpeech(allow_verb_noun_swap=True))\n",
    "    \n",
    "    #\n",
    "    # Universal Sentence Encoder with a minimum angular similarity of Îµ = 0.5.\n",
    "    #\n",
    "    # In the TextFooler code, they forget to divide the angle between the two\n",
    "    # embeddings by pi. So if the original threshold was that 1 - sim >= 0.5, the\n",
    "    # new threshold is 1 - (0.5) / pi = 0.840845057\n",
    "    #\n",
    "    use_constraint = UniversalSentenceEncoder(\n",
    "        threshold=0.84,\n",
    "        metric=\"angular\",\n",
    "        compare_against_original=False,\n",
    "        window_size=15,\n",
    "        skip_text_shorter_than_window=True,\n",
    "    )\n",
    "    constraints.append(use_constraint)\n",
    "    #\n",
    "    # Goal is untargeted classification\n",
    "    #\n",
    "    goal_function = UntargetedClassification(model)\n",
    "    #\n",
    "    # Greedily swap words with \"Word Importance Ranking\".\n",
    "    #\n",
    "    search_method = GreedyWordSwapWIR(wir_method=\"delete\")\n",
    "\n",
    "    return Attack(goal_function, constraints, transformation, search_method)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "applicable-section",
   "metadata": {},
   "outputs": [],
   "source": [
    "#recipe = TextFoolerJin2019(\n",
    "#    goal_function=UntargetedClassification(model), \n",
    "#    search_method=GreedyWordSwapWIR(wir_method=\"delete\"),\n",
    "#    transformation=WordSwapEmbedding(max_candidates=50),\n",
    "#)\n",
    "#recipe.constraints = constraints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "deluxe-cambridge",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "result_df = pd.DataFrame({'attack_result': [], 'cross_val_batch': []})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "waiting-practice",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_names = [f'ru_train_crossval_{fold_id}' for fold_id in range(5)]\n",
    "model_names = [f'xlm_roberta_model_ru_crossval_{fold_id}' for fold_id in range(5)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bearing-glenn",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 LABELS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "- This IS expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Using /home/mlepekhin/textattack_experiments/tfhub_modules to cache modules.\n",
      "Downloading TF-Hub Module 'https://tfhub.dev/google/universal-sentence-encoder/4'.\n",
      "Downloading https://tfhub.dev/google/universal-sentence-encoder/4: 270.00MB\n",
      "Downloading https://tfhub.dev/google/universal-sentence-encoder/4: 540.00MB\n",
      "Downloading https://tfhub.dev/google/universal-sentence-encoder/4: 810.00MB\n",
      "Downloaded https://tfhub.dev/google/universal-sentence-encoder/4, Total size: 987.47MB\n",
      "Downloaded TF-Hub Module 'https://tfhub.dev/google/universal-sentence-encoder/4'.\n",
      "\u001b[34;1mtextattack\u001b[0m: Unknown if model of class <class 'transformers.models.xlm_roberta.modeling_xlm_roberta.XLMRobertaForSequenceClassification'> compatible with goal function <class 'textattack.goal_functions.classification.untargeted_classification.UntargetedClassification'>.\n",
      "290it [39:17,  8.13s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 LABELS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "- This IS expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "\u001b[34;1mtextattack\u001b[0m: Unknown if model of class <class 'transformers.models.xlm_roberta.modeling_xlm_roberta.XLMRobertaForSequenceClassification'> compatible with goal function <class 'textattack.goal_functions.classification.untargeted_classification.UntargetedClassification'>.\n",
      "289it [1:18:58, 16.39s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 LABELS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "- This IS expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "\u001b[34;1mtextattack\u001b[0m: Unknown if model of class <class 'transformers.models.xlm_roberta.modeling_xlm_roberta.XLMRobertaForSequenceClassification'> compatible with goal function <class 'textattack.goal_functions.classification.untargeted_classification.UntargetedClassification'>.\n",
      "290it [52:17, 10.82s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 LABELS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "- This IS expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "\u001b[34;1mtextattack\u001b[0m: Unknown if model of class <class 'transformers.models.xlm_roberta.modeling_xlm_roberta.XLMRobertaForSequenceClassification'> compatible with goal function <class 'textattack.goal_functions.classification.untargeted_classification.UntargetedClassification'>.\n",
      "289it [1:14:11, 15.40s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 LABELS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "- This IS expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "\u001b[34;1mtextattack\u001b[0m: Unknown if model of class <class 'transformers.models.xlm_roberta.modeling_xlm_roberta.XLMRobertaForSequenceClassification'> compatible with goal function <class 'textattack.goal_functions.classification.untargeted_classification.UntargetedClassification'>.\n",
      "289it [1:17:50, 16.16s/it]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "for batch_id, (dataset_name, model_name) in enumerate(zip(dataset_names, model_names)):\n",
    "    cur_dataset = __import__(dataset_name).dataset\n",
    "    cur_model = __import__(model_name).model\n",
    "    attacker = build_textfooler(cur_model)\n",
    "    for idx, result in tqdm(enumerate(attacker.attack_dataset(cur_dataset))):\n",
    "        result_df = result_df.append({\n",
    "            'attack_result': result.__str__(color_method='html'),\n",
    "            'cross_val_batch': batch_id\n",
    "        }, ignore_index=True)\n",
    "        #print(('x' * 20), f'Result {idx+1}', ('x' * 20))\n",
    "        #print(result.__str__(color_method='html'))\n",
    "        #print(len(str(result).split('\\n\\n')))\n",
    "    result_df.to_csv('new_ru_xlm_roberta_cross_val_result_50.csv')\n",
    "    del cur_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "russian-silicon",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1447\n"
     ]
    }
   ],
   "source": [
    "print(len(result_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "assigned-spanish",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "sharp-australian",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_color_tags(text):\n",
    "    text = re.sub(r'<font color = [a-zA-Z]+>', ' ', text)\n",
    "    text = re.sub(r'</font>', ' ', text)\n",
    "    text = text.replace('  ', ' ')\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "animated-milan",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['A14', 'A9', 'A8', 'A8', 'A1', 'A11', 'A16', 'A11', 'A9', 'A12', 'A14', 'A12', 'A1', 'A1', 'A12', 'A9', 'A8', 'A11', 'A14', 'A12', 'A8', 'A4', 'A1', 'A17', 'A16', 'A8', 'A12', 'A8', 'A1', 'A11', 'A8', 'A1', 'A11', 'A7', 'A1', 'A8', 'A16', 'A16', 'A1', 'A14', 'A8', 'A7', 'A8', 'A12', 'A16', 'A4', 'A11', 'A14', 'A14', 'A9', 'A8', 'A14', 'A8', 'A12', 'A11', 'A14', 'A14', 'A8', 'A14', 'A9', 'A8', 'A8', 'A16', 'A12', 'A1', 'A8', 'A14', 'A8', 'A17', 'A14', 'A1', 'A14', 'A12', 'A1', 'A14', 'A9', 'A11', 'A7', 'A1', 'A14', 'A17', 'A12', 'A14', 'A8', 'A1', 'A12', 'A8', 'A1', 'A4', 'A8', 'A1', 'A7', 'A9', 'A8', 'A17', 'A7', 'A14', 'A9', 'A12', 'A7', 'A12', 'A8', 'A9', 'A1', 'A8', 'A8', 'A4', 'A1', 'A1', 'A8', 'A8', 'A8', 'A8', 'A12', 'A12', 'A14', 'A7', 'A8', 'A8', 'A9', 'A8', 'A1', 'A16', 'A9', 'A17', 'A4', 'A12', 'A16', 'A11', 'A16', 'A1', 'A9', 'A8', 'A4', 'A14', 'A12', 'A17', 'A17', 'A11', 'A8', 'A16', 'A12', 'A14', 'A11', 'A8', 'A14', 'A1', 'A14', 'A14', 'A8', 'A1', 'A14', 'A12', 'A8', 'A11', 'A11', 'A17', 'A4', 'A1', 'A14', 'A12', 'A12', 'A12', 'A8', 'A16', 'A12', 'A7', 'A11', 'A12', 'A14', 'A12', 'A1', 'A9', 'A11', 'A12', 'A8', 'A4', 'A14', 'A8', 'A12', 'A14', 'A12', 'A17', 'A9', 'A14', 'A12', 'A4', 'A14', 'A1', 'A14', 'A14', 'A1', 'A11', 'A11', 'A9', 'A8', 'A8', 'A11', 'A14', 'A7', 'A17', 'A12', 'A16', 'A1', 'A17', 'A11', 'A9', 'A11', 'A8', 'A4', 'A1', 'A1', 'A12', 'A11', 'A14', 'A8', 'A17', 'A12', 'A16', 'A8', 'A11', 'A8', 'A8', 'A8', 'A1', 'A8', 'A4', 'A14', 'A12', 'A11', 'A1', 'A9', 'A8', 'A12', 'A8', 'A4', 'A8', 'A1', 'A1', 'A8', 'A12', 'A16', 'A14', 'A8', 'A16', 'A8', 'A12', 'A17', 'A8', 'A1', 'A12', 'A14', 'A12', 'A12', 'A17', 'A17', 'A14', 'A12', 'A8', 'A12', 'A12', 'A14', 'A17', 'A1', 'A11', 'A1', 'A8', 'A8', 'A14', 'A8', 'A12', 'A17', 'A12', 'A12', 'A14', 'A12', 'A8', 'A1', 'A14', 'A12', 'A1', 'A12', 'A1', 'A12', 'A8', 'A11', 'A11', 'A11', 'A11', 'A17', 'A1', 'A8', 'A8', 'A1', 'A11', 'A8', 'A1', 'A17', 'A1', 'A14', 'A8', 'A8', 'A1', 'A8', 'A8', 'A1', 'A8', 'A8', 'A7', 'A12', 'A16', 'A12', 'A8', 'A8', 'A12', 'A11', 'A4', 'A9', 'A8', 'A12', 'A12', 'A4', 'A4', 'A9', 'A14', 'A9', 'A12', 'A17', 'A8', 'A1', 'A9', 'A16', 'A1', 'A12', 'A11', 'A14', 'A17', 'A12', 'A12', 'A1', 'A12', 'A12', 'A14', 'A17', 'A11', 'A1', 'A8', 'A8', 'A14', 'A17', 'A12', 'A8', 'A8', 'A17', 'A11', 'A11', 'A8', 'A1', 'A11', 'A4', 'A8', 'A8', 'A8', 'A7', 'A17', 'A11', 'A1', 'A8', 'A8', 'A7', 'A1', 'A7', 'A9', 'A7', 'A8', 'A4', 'A12', 'A8', 'A9', 'A12', 'A1', 'A9', 'A17', 'A8', 'A1', 'A8', 'A12', 'A8', 'A8', 'A17', 'A14', 'A9', 'A8', 'A17', 'A12', 'A9', 'A14', 'A8', 'A8', 'A16', 'A1', 'A12', 'A1', 'A14', 'A8', 'A14', 'A1', 'A7', 'A12', 'A17', 'A17', 'A1', 'A1', 'A1', 'A8', 'A12', 'A8', 'A8', 'A1', 'A17', 'A12', 'A1', 'A11', 'A14', 'A8', 'A8', 'A1', 'A11', 'A1', 'A16', 'A12', 'A14', 'A1', 'A7', 'A1', 'A1', 'A11', 'A8', 'A12', 'A1', 'A12', 'A4', 'A8', 'A7', 'A11', 'A11', 'A7', 'A11', 'A11', 'A12', 'A1', 'A4', 'A14', 'A8', 'A1', 'A12', 'A1', 'A7', 'A8', 'A8', 'A8', 'A9', 'A8', 'A8', 'A14', 'A8', 'A11', 'A17', 'A14', 'A12', 'A1', 'A16', 'A16', 'A1', 'A8', 'A14', 'A11', 'A14', 'A8', 'A7', 'A11', 'A8', 'A8', 'A16', 'A1', 'A12', 'A8', 'A1', 'A8', 'A9', 'A14', 'A1', 'A8', 'A17', 'A1', 'A9', 'A16', 'A17', 'A7', 'A17', 'A16', 'A4', 'A1', 'A11', 'A8', 'A4', 'A8', 'A8', 'A8', 'A11', 'A11', 'A1', 'A12', 'A9', 'A1', 'A8', 'A7', 'A11', 'A8', 'A16', 'A11', 'A8', 'A12', 'A14', 'A17', 'A8', 'A8', 'A17', 'A1', 'A14', 'A11', 'A8', 'A11', 'A1', 'A1', 'A11', 'A8', 'A17', 'A1', 'A14', 'A9', 'A1', 'A16', 'A12', 'A9', 'A4', 'A12', 'A8', 'A11', 'A11', 'A1', 'A8', 'A17', 'A7', 'A8', 'A12', 'A7', 'A16', 'A9', 'A11', 'A8', 'A7', 'A14', 'A11', 'A4', 'A12', 'A11', 'A11', 'A1', 'A1', 'A12', 'A8', 'A4', 'A8', 'A1', 'A11', 'A8', 'A4', 'A12', 'A12', 'A7', 'A8', 'A8', 'A16', 'A16', 'A11', 'A8', 'A4', 'A8', 'A14', 'A1', 'A11', 'A14', 'A14', 'A1', 'A8', 'A9', 'A11', 'A16', 'A1', 'A12', 'A8', 'A14', 'A1', 'A12', 'A14', 'A12', 'A12', 'A1', 'A11', 'A14', 'A16', 'A8', 'A8', 'A17', 'A12', 'A17', 'A4', 'A17', 'A9', 'A4', 'A12', 'A8', 'A17', 'A12', 'A14', 'A4', 'A1', 'A12', 'A17', 'A9', 'A1', 'A8', 'A17', 'A16', 'A12', 'A14', 'A11', 'A12', 'A14', 'A8', 'A12', 'A8', 'A11', 'A7', 'A8', 'A9', 'A8', 'A8', 'A12', 'A14', 'A1', 'A8', 'A9', 'A12', 'A12', 'A16', 'A8', 'A9', 'A12', 'A12', 'A8', 'A12', 'A1', 'A12', 'A8', 'A17', 'A11', 'A11', 'A9', 'A17', 'A1', 'A1', 'A1', 'A9', 'A1', 'A4', 'A12', 'A8', 'A14', 'A17', 'A12', 'A8', 'A17', 'A11', 'A1', 'A4', 'A16', 'A8', 'A12', 'A17', 'A1', 'A8', 'A8', 'A8', 'A14', 'A1', 'A8', 'A8', 'A8', 'A14', 'A12', 'A12', 'A8', 'A17', 'A12', 'A8', 'A12', 'A8', 'A1', 'A17', 'A12', 'A8', 'A8', 'A8', 'A12', 'A8', 'A17', 'A7', 'A7', 'A14', 'A14', 'A8', 'A1', 'A8', 'A11', 'A14', 'A17', 'A12', 'A12', 'A8', 'A8', 'A1', 'A11', 'A8', 'A12', 'A7', 'A14', 'A11', 'A8', 'A14', 'A1', 'A8', 'A14', 'A1', 'A14', 'A12', 'A12', 'A12', 'A14', 'A8', 'A17', 'A8', 'A17', 'A8', 'A8', 'A4', 'A9', 'A8', 'A8', 'A14', 'A9', 'A14', 'A11', 'A12', 'A9', 'A8', 'A16', 'A12', 'A1', 'A8', 'A12', 'A8', 'A8', 'A17', 'A11', 'A7', 'A8', 'A9', 'A7', 'A1', 'A12', 'A8', 'A1', 'A11', 'A17', 'A12', 'A12', 'A1', 'A14', 'A17', 'A8', 'A8', 'A12', 'A8', 'A8', 'A4', 'A8', 'A1', 'A7', 'A12', 'A11', 'A1', 'A1', 'A1', 'A8', 'A12', 'A4', 'A7', 'A12', 'A4', 'A8', 'A12', 'A8', 'A8', 'A16', 'A8', 'A12', 'A11', 'A8', 'A11', 'A12', 'A8', 'A17', 'A16', 'A9', 'A12', 'A8', 'A1', 'A17', 'A14', 'A8', 'A7', 'A8', 'A8', 'A8', 'A8', 'A1', 'A17', 'A9', 'A8', 'A8', 'A14', 'A8', 'A12', 'A12', 'A11', 'A16', 'A14', 'A4', 'A11', 'A16', 'A14', 'A8', 'A8', 'A16', 'A11', 'A1', 'A12', 'A14', 'A12', 'A12', 'A1', 'A8', 'A16', 'A1', 'A12', 'A4', 'A16', 'A8', 'A8', 'A16', 'A8', 'A17', 'A17', 'A12', 'A12', 'A17', 'A1', 'A8', 'A11', 'A11', 'A16', 'A8', 'A1', 'A4', 'A12', 'A11', 'A11', 'A8', 'A12', 'A9', 'A8', 'A8', 'A7', 'A17', 'A11', 'A12', 'A11', 'A11', 'A4', 'A14', 'A12', 'A7', 'A9', 'A8', 'A17', 'A9', 'A14', 'A14', 'A11', 'A8', 'A12', 'A16', 'A8', 'A1', 'A16', 'A8', 'A8', 'A8', 'A16', 'A1', 'A8', 'A4', 'A17', 'A8', 'A8', 'A4', 'A16', 'A12', 'A16', 'A8', 'A8', 'A1', 'A12', 'A8', 'A17', 'A7', 'A8', 'A8', 'A16', 'A8', 'A8', 'A8', 'A8', 'A12', 'A12', 'A8', 'A14', 'A12', 'A4', 'A11', 'A12', 'A8', 'A14', 'A14', 'A14', 'A8', 'A12', 'A8', 'A8', 'A8', 'A7', 'A1', 'A4', 'A8', 'A16', 'A1', 'A8', 'A12', 'A12', 'A16', 'A8', 'A8', 'A11', 'A1', 'A8', 'A8', 'A1', 'A1', 'A11', 'A4', 'A8', 'A8', 'A8', 'A8', 'A11', 'A8', 'A8', 'A17', 'A16', 'A8', 'A8', 'A12', 'A8', 'A12', 'A11', 'A14', 'A1', 'A12', 'A1', 'A1', 'A8', 'A1', 'A8', 'A12', 'A8', 'A11', 'A1', 'A1', 'A12', 'A12', 'A12', 'A8', 'A14', 'A1', 'A12', 'A12', 'A16', 'A8', 'A1', 'A9', 'A14', 'A12', 'A8', 'A11', 'A9', 'A8', 'A1', 'A8', 'A8', 'A9', 'A11', 'A14', 'A1', 'A8', 'A12', 'A4', 'A14', 'A14', 'A4', 'A8', 'A17', 'A12', 'A9', 'A8', 'A8', 'A17', 'A12', 'A16', 'A7', 'A9', 'A7', 'A14', 'A1', 'A8', 'A8', 'A1', 'A8', 'A12', 'A1', 'A1', 'A1', 'A8', 'A9', 'A1', 'A12', 'A1', 'A17', 'A8', 'A4', 'A9', 'A1', 'A11', 'A8', 'A8', 'A4', 'A8', 'A16', 'A12', 'A9', 'A1', 'A14', 'A17', 'A12', 'A4', 'A7', 'A12', 'A16', 'A17', 'A7', 'A16', 'A16', 'A8', 'A17', 'A12', 'A1', 'A1', 'A14', 'A9', 'A8', 'A12', 'A8', 'A14', 'A12', 'A12', 'A8', 'A17', 'A14', 'A16', 'A11', 'A8', 'A16', 'A7', 'A12', 'A8', 'A8', 'A8', 'A1', 'A4', 'A14', 'A9', 'A8', 'A1', 'A4', 'A7', 'A1', 'A1', 'A4', 'A8', 'A1', 'A8', 'A14', 'A1', 'A14', 'A12', 'A8', 'A12', 'A14', 'A8', 'A4', 'A1', 'A17', 'A8', 'A8', 'A1', 'A12', 'A17', 'A14', 'A11', 'A14', 'A11', 'A8', 'A14', 'A12', 'A8', 'A12', 'A12', 'A8', 'A1', 'A8', 'A8', 'A1', 'A8', 'A8', 'A17', 'A8', 'A17', 'A14', 'A17', 'A14', 'A8', 'A1', 'A8', 'A17', 'A1', 'A11', 'A16', 'A11', 'A11', 'A1', 'A7', 'A16', 'A7', 'A12', 'A8', 'A8', 'A1', 'A12', 'A1', 'A11', 'A8', 'A12', 'A9', 'A1', 'A1', 'A1', 'A8', 'A17', 'A8', 'A14', 'A14', 'A12', 'A8', 'A14', 'A1', 'A1', 'A1', 'A4', 'A4', 'A9', 'A11', 'A17', 'A16', 'A17', 'A12', 'A8', 'A1', 'A14', 'A11', 'A4', 'A16', 'A8', 'A14', 'A1', 'A12', 'A8', 'A12', 'A17', 'A1', 'A12', 'A8', 'A17', 'A1', 'A8', 'A12', 'A17', 'A12', 'A8', 'A11', 'A8', 'A12', 'A8', 'A12', 'A14', 'A12', 'A8', 'A14', 'A7', 'A12', 'A17', 'A8', 'A16', 'A8', 'A17', 'A11', 'A14', 'A1', 'A17', 'A1', 'A8', 'A8', 'A8', 'A11', 'A8', 'A11', 'A8', 'A9', 'A12', 'A8', 'A8', 'A12', 'A7', 'A1', 'A9', 'A17', 'A17', 'A14', 'A8', 'A17', 'A8', 'A8', 'A4', 'A1', 'A12', 'A8', 'A8', 'A8', 'A14', 'A8', 'A11', 'A8', 'A1', 'A14', 'A17', 'A8', 'A12', 'A12', 'A7', 'A1', 'A17', 'A8', 'A8', 'A8', 'A8', 'A12', 'A1', 'A1', 'A7', 'A12', 'A8', 'A16', 'A14', 'A8', 'A14', 'A8', 'A11', 'A8', 'A8', 'A12', 'A8', 'A8', 'A1', 'A17', 'A8', 'A1', 'A14', 'A17', 'A11', 'A12', 'A14', 'A16', 'A8', 'A14', 'A1', 'A4', 'A11', 'A12', 'A8', 'A11', 'A8', 'A8', 'A14', 'A9', 'A16', 'A1', 'A9', 'A16', 'A8', 'A14', 'A7', 'A17', 'A12', 'A8', 'A8', 'A14', 'A1', 'A17', 'A17', 'A12', 'A8', 'A7', 'A1', 'A14', 'A11', 'A7', 'A8', 'A8', 'A9', 'A8', 'A1', 'A1', 'A12', 'A1', 'A8', 'A12', 'A7', 'A8', 'A11', 'A8', 'A17', 'A12', 'A14', 'A8', 'A8', 'A1', 'A12', 'A14', 'A12', 'A8', 'A12', 'A14', 'A1', 'A8', 'A1', 'A8', 'A11', 'A8', 'A11', 'A11', 'A17', 'A9', 'A1', 'A12', 'A8', 'A14', 'A12', 'A1', 'A8', 'A4', 'A12', 'A16', 'A12', 'A17', 'A14', 'A8', 'A7', 'A1', 'A4', 'A4', 'A12', 'A17', 'A8', 'A1', 'A11', 'A7', 'A11', 'A8', 'A7', 'A11', 'A1', 'A8', 'A4', 'A8', 'A9', 'A12', 'A17', 'A1', 'A8', 'A8', 'A11', 'A16', 'A8', 'A12', 'A7', 'A1', 'A1']\n"
     ]
    }
   ],
   "source": [
    "all_targets = []\n",
    "\n",
    "for dataset_name in dataset_names:\n",
    "    dataset_labels = [id_to_label[pair[1]] for pair in __import__(dataset_name).dataset]\n",
    "    all_targets = all_targets + dataset_labels\n",
    "print(all_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "applicable-representative",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "745\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_words</th>\n",
       "      <th>median</th>\n",
       "      <th>model_label_new</th>\n",
       "      <th>model_label_old</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>17.545455</td>\n",
       "      <td>16.0</td>\n",
       "      <td>A9</td>\n",
       "      <td>A14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20.500000</td>\n",
       "      <td>8.5</td>\n",
       "      <td>A1</td>\n",
       "      <td>A9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>25.392157</td>\n",
       "      <td>19.0</td>\n",
       "      <td>A1</td>\n",
       "      <td>A8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>12.000000</td>\n",
       "      <td>11.5</td>\n",
       "      <td>A16</td>\n",
       "      <td>A1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>14.914286</td>\n",
       "      <td>12.0</td>\n",
       "      <td>A7</td>\n",
       "      <td>A12</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean_words  median model_label_new model_label_old\n",
       "0   17.545455    16.0              A9             A14\n",
       "1   20.500000     8.5              A1              A9\n",
       "2   25.392157    19.0              A1              A8\n",
       "3   12.000000    11.5             A16              A1\n",
       "4   14.914286    12.0              A7             A12"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "ru_attacked_df = pd.DataFrame()\n",
    "attack_count = collections.defaultdict(int)\n",
    "word_count = collections.defaultdict(list)\n",
    "good_count = 0\n",
    "\n",
    "for raw_text, real_target in zip(result_df.attack_result, all_targets):\n",
    "    if len(raw_text.split('\\n\\n')) == 3:\n",
    "        header, old, new = raw_text.split('\\n\\n')\n",
    "        _, label_id1, label_id2 = re.split(r'<font color = [a-zA-Z]+>', header)\n",
    "        label1 = id_to_label[int(label_id1.split()[0])]\n",
    "        label2 = id_to_label[int(label_id2.split()[0])]\n",
    "        changed_words = len(tuple(re.finditer(r'<font color = [a-zA-Z]+>', old)))\n",
    "        word_count[(label1, label2)].append(changed_words)\n",
    "        attack_count[(label1, label2)] += 1\n",
    "        \n",
    "        \n",
    "        ru_attacked_df = ru_attacked_df.append(\n",
    "            {'old_text': remove_color_tags(old), \n",
    "             'text': remove_color_tags(new),\n",
    "             'changed_words_num': changed_words,\n",
    "             'old_model_target': label1,\n",
    "             'new_model_target': label2,\n",
    "             'target': real_target\n",
    "            },\n",
    "            ignore_index=True\n",
    "        )\n",
    "        good_count += 1\n",
    "print(good_count)\n",
    "\n",
    "clean_stat_df = pd.DataFrame()\n",
    "for (label1, label2) in attack_count.keys():\n",
    "    cur_attack_count = attack_count[(label1, label2)]\n",
    "    cur_word_count = word_count[(label1, label2)]\n",
    "    clean_stat_df = clean_stat_df.append(\n",
    "        {'model_label_old': label1, \n",
    "         'model_label_new': label2, \n",
    "         'mean_words': sum(cur_word_count) / cur_attack_count, \n",
    "         'median': np.median(cur_word_count)\n",
    "        }, \n",
    "        ignore_index=True\n",
    "    )\n",
    "clean_stat_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "municipal-affect",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "745\n"
     ]
    }
   ],
   "source": [
    "print(good_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "valuable-phrase",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_stat_df.to_csv('/home/mlepekhin/data/new_ru_textfooler_stat_50.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "blessed-command",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>changed_words_num</th>\n",
       "      <th>new_model_target</th>\n",
       "      <th>old_model_target</th>\n",
       "      <th>old_text</th>\n",
       "      <th>target</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3.0</td>\n",
       "      <td>A9</td>\n",
       "      <td>A14</td>\n",
       "      <td>ÐÐ¾ÑÐ¾Ð¶Ð¸Ðµ ÑÐµÐ¼Ñ Ð½Ð°ÑÑÐ½ÑÑ  ÑÐ°Ð±Ð¾Ñ Ð¿Ð¾ Ð¸ÑÑÐ¾ÑÐ¸Ð¸ Ð¸ Ð¸ÑÑÐ¾Ñ...</td>\n",
       "      <td>A14</td>\n",
       "      <td>ÐÐ¾ÑÐ¾Ð¶Ð¸Ðµ ÑÐµÐ¼Ñ ÑÑÐµÐ±Ð½Ð¸ÐºÐ¾Ð²  ÐºÐ½Ð¸Ð³ Ð¿Ð¾ Ð¸ÑÑÐ¾ÑÐ¸Ð¸ Ð¸ Ð¸ÑÑÐ¾...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>16.0</td>\n",
       "      <td>A1</td>\n",
       "      <td>A9</td>\n",
       "      <td>Ð£Ð³Ð¾Ð»Ð¾Ð²Ð½ÑÐ¹ ÐºÐ¾Ð´ÐµÐºÑ ( Ð£Ð  Ð Ð¤ ) ÐÐ±ÑÐ°Ñ ÑÐ°ÑÑÑ Ð Ð°Ð·Ð´ÐµÐ»...</td>\n",
       "      <td>A9</td>\n",
       "      <td>Ð£Ð³Ð¾Ð»Ð¾Ð²Ð½ÑÐ¹ Ð·Ð°ÐºÐ¾Ð½Ð¾Ð¿ÑÐ¾ÐµÐºÑ ( Ð¡Ð¢  ÐÐÐÐ­ÐÐÐÐÐÐ ÐÐÐÐÐ¢ÐÐ¯...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.0</td>\n",
       "      <td>A1</td>\n",
       "      <td>A8</td>\n",
       "      <td>2100 ÐµÐ³Ð¸Ð¿ÐµÑÑÐºÐ¸Ñ ÑÐ¾Ð»Ð´Ð°Ñ Ð¿ÑÐ¸ÑÐ¾ÐµÐ´Ð¸Ð½ÑÑÑÑ Ðº Ð¼Ð¸ÑÐ¾ÑÐ²Ð¾...</td>\n",
       "      <td>A8</td>\n",
       "      <td>2100 ÐµÐ³Ð¸Ð¿ÐµÑÑÐºÐ¸Ñ ÑÐ¾Ð»Ð´Ð°Ñ Ð¿ÑÐ¸ÑÐ¾ÐµÐ´Ð¸Ð½ÑÑÑÑ Ðº Ð¼Ð¸ÑÐ¾ÑÐ²Ð¾...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.0</td>\n",
       "      <td>A1</td>\n",
       "      <td>A8</td>\n",
       "      <td>ÐÐ¸Ð´ÐµÑ ÑÑÐ°Ð½ÑÑÐ·ÑÐºÐ¾Ð¹ Ð¿Ð°ÑÑÐ¸Ð¸ Â« ÐÐ°ÑÐ¸Ð¾Ð½Ð°Ð»ÑÐ½ÑÐ¹ ÑÑÐ¾Ð½Ñ...</td>\n",
       "      <td>A8</td>\n",
       "      <td>Ð Ð°ÑÐºÐ¾Ð»Ð° ÑÑÐ°Ð½ÑÑÐ·ÑÐºÐ¾Ð¹ Ð¿Ð°ÑÑÐ¸Ð¸ Â« ÐÐ°ÑÐ¸Ð¾Ð½Ð°Ð»ÑÐ½ÑÐ¹ ÑÑÐ¾...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9.0</td>\n",
       "      <td>A16</td>\n",
       "      <td>A1</td>\n",
       "      <td>ÐÐµÑÐ²Ð°Ñ Ð¿Ð¾Ð»Ð¾Ð²Ð¸Ð½Ð° Ð´Ð²Ð°Ð´ÑÐ°ÑÐ¾Ð³Ð¾ Ð²ÐµÐºÐ° Ð±ÑÐ»Ð° Ð¿Ð¾Ð»Ð½Ð¾Ð¹ ÐºÐ°...</td>\n",
       "      <td>A1</td>\n",
       "      <td>ÐÐµÑÐ²Ð°Ñ Ð¿Ð¾Ð»Ð¾Ð²Ð¸Ð½Ð° Ð´Ð²Ð°Ð´ÑÐ°ÑÐ¾Ð³Ð¾ Ð²ÐµÐºÐ° Ð±ÑÐ»Ð° Ð¿Ð¾Ð»Ð½Ð¾Ð¹ ÐºÐ°...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   changed_words_num new_model_target old_model_target  \\\n",
       "0                3.0               A9              A14   \n",
       "1               16.0               A1               A9   \n",
       "2                3.0               A1               A8   \n",
       "3                3.0               A1               A8   \n",
       "4                9.0              A16               A1   \n",
       "\n",
       "                                            old_text target  \\\n",
       "0  ÐÐ¾ÑÐ¾Ð¶Ð¸Ðµ ÑÐµÐ¼Ñ Ð½Ð°ÑÑÐ½ÑÑ  ÑÐ°Ð±Ð¾Ñ Ð¿Ð¾ Ð¸ÑÑÐ¾ÑÐ¸Ð¸ Ð¸ Ð¸ÑÑÐ¾Ñ...    A14   \n",
       "1  Ð£Ð³Ð¾Ð»Ð¾Ð²Ð½ÑÐ¹ ÐºÐ¾Ð´ÐµÐºÑ ( Ð£Ð  Ð Ð¤ ) ÐÐ±ÑÐ°Ñ ÑÐ°ÑÑÑ Ð Ð°Ð·Ð´ÐµÐ»...     A9   \n",
       "2  2100 ÐµÐ³Ð¸Ð¿ÐµÑÑÐºÐ¸Ñ ÑÐ¾Ð»Ð´Ð°Ñ Ð¿ÑÐ¸ÑÐ¾ÐµÐ´Ð¸Ð½ÑÑÑÑ Ðº Ð¼Ð¸ÑÐ¾ÑÐ²Ð¾...     A8   \n",
       "3   ÐÐ¸Ð´ÐµÑ ÑÑÐ°Ð½ÑÑÐ·ÑÐºÐ¾Ð¹ Ð¿Ð°ÑÑÐ¸Ð¸ Â« ÐÐ°ÑÐ¸Ð¾Ð½Ð°Ð»ÑÐ½ÑÐ¹ ÑÑÐ¾Ð½Ñ...     A8   \n",
       "4  ÐÐµÑÐ²Ð°Ñ Ð¿Ð¾Ð»Ð¾Ð²Ð¸Ð½Ð° Ð´Ð²Ð°Ð´ÑÐ°ÑÐ¾Ð³Ð¾ Ð²ÐµÐºÐ° Ð±ÑÐ»Ð° Ð¿Ð¾Ð»Ð½Ð¾Ð¹ ÐºÐ°...     A1   \n",
       "\n",
       "                                                text  \n",
       "0  ÐÐ¾ÑÐ¾Ð¶Ð¸Ðµ ÑÐµÐ¼Ñ ÑÑÐµÐ±Ð½Ð¸ÐºÐ¾Ð²  ÐºÐ½Ð¸Ð³ Ð¿Ð¾ Ð¸ÑÑÐ¾ÑÐ¸Ð¸ Ð¸ Ð¸ÑÑÐ¾...  \n",
       "1  Ð£Ð³Ð¾Ð»Ð¾Ð²Ð½ÑÐ¹ Ð·Ð°ÐºÐ¾Ð½Ð¾Ð¿ÑÐ¾ÐµÐºÑ ( Ð¡Ð¢  ÐÐÐÐ­ÐÐÐÐÐÐ ÐÐÐÐÐ¢ÐÐ¯...  \n",
       "2  2100 ÐµÐ³Ð¸Ð¿ÐµÑÑÐºÐ¸Ñ ÑÐ¾Ð»Ð´Ð°Ñ Ð¿ÑÐ¸ÑÐ¾ÐµÐ´Ð¸Ð½ÑÑÑÑ Ðº Ð¼Ð¸ÑÐ¾ÑÐ²Ð¾...  \n",
       "3   Ð Ð°ÑÐºÐ¾Ð»Ð° ÑÑÐ°Ð½ÑÑÐ·ÑÐºÐ¾Ð¹ Ð¿Ð°ÑÑÐ¸Ð¸ Â« ÐÐ°ÑÐ¸Ð¾Ð½Ð°Ð»ÑÐ½ÑÐ¹ ÑÑÐ¾...  \n",
       "4  ÐÐµÑÐ²Ð°Ñ Ð¿Ð¾Ð»Ð¾Ð²Ð¸Ð½Ð° Ð´Ð²Ð°Ð´ÑÐ°ÑÐ¾Ð³Ð¾ Ð²ÐµÐºÐ° Ð±ÑÐ»Ð° Ð¿Ð¾Ð»Ð½Ð¾Ð¹ ÐºÐ°...  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ru_attacked_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "printable-donor",
   "metadata": {},
   "outputs": [],
   "source": [
    "ru_attacked_df.to_csv('/home/mlepekhin/data/new_ru_attacked_50.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "offensive-white",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' ÐÐ¸Ð¶Ð½ÐµÐºÐ°Ð¼ÑÐºÐ½ÐµÑÑÐµÑÐ¸Ð¼'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "remove_color_tags('<font color = gray>ÐÐ¸Ð¶Ð½ÐµÐºÐ°Ð¼ÑÐºÐ½ÐµÑÑÐµÑÐ¸Ð¼')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "scenic-elite",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
