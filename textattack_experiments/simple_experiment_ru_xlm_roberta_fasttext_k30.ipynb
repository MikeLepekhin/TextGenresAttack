{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "opening-rings",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "- This IS expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from ru_test import dataset, labels\n",
    "from ru_xlm_roberta_model import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "distant-decimal",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_to_id = {label: label_id for label_id, label in enumerate(labels)}\n",
    "id_to_label = {label_id: label for label_id, label in enumerate(labels)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "single-combine",
   "metadata": {},
   "outputs": [],
   "source": [
    "from textattack.constraints.grammaticality import PartOfSpeech\n",
    "from textattack.constraints.pre_transformation import (\n",
    "    InputColumnModification,\n",
    "    RepeatModification,\n",
    "    StopwordModification,\n",
    ")\n",
    "from textattack.constraints.semantics import WordEmbeddingDistance\n",
    "from textattack.constraints.semantics.sentence_encoders import UniversalSentenceEncoder\n",
    "\n",
    "from textattack.attack_recipes import TextFoolerJin2019\n",
    "from textattack.goal_functions import UntargetedClassification\n",
    "from textattack.search_methods import GreedyWordSwapWIR\n",
    "from textattack.transformations import WordSwapEmbedding\n",
    "from textattack.shared.attack import Attack\n",
    "\n",
    "from textattack.shared import WordEmbedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "boolean-parliament",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "embedding_matrix = np.load('ru_fasttext_30000/embeddings_matrix.npy')\n",
    "nn_matrix = np.load('ru_fasttext_30000/nn_matrix.npy')\n",
    "index2word = pickle.load(open('ru_fasttext_30000/index2word.pcl', 'rb'))\n",
    "word2index = pickle.load(open('ru_fasttext_30000/word2index.pcl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "accepted-conference",
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "\n",
    "smart_word2index = collections.defaultdict(int, word2index)\n",
    "smart_index2word = collections.defaultdict(str, index2word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "endangered-procurement",
   "metadata": {},
   "outputs": [],
   "source": [
    "ru_fasttext_embedding = WordEmbedding(embedding_matrix, word2index, index2word, nn_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "offshore-senior",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_textfooler(model):\n",
    "    #\n",
    "    # Swap words with their 50 closest embedding nearest-neighbors.\n",
    "    # Embedding: Counter-fitted PARAGRAM-SL999 vectors.\n",
    "    #\n",
    "    transformation = WordSwapEmbedding(max_candidates=30, embedding=ru_fasttext_embedding)\n",
    "    #\n",
    "    # Don't modify the same word twice or the stopwords defined\n",
    "    # in the TextFooler public implementation.\n",
    "    #\n",
    "    # fmt: off\n",
    "    stopwords = set(\n",
    "    #    [\"a\", \"about\", \"above\", \"across\", \"after\", \"afterwards\", \"again\", \"against\", \"ain\", \"all\", \"almost\", \"alone\", \"along\", \"already\", \"also\", \"although\", \"am\", \"among\", \"amongst\", \"an\", \"and\", \"another\", \"any\", \"anyhow\", \"anyone\", \"anything\", \"anyway\", \"anywhere\", \"are\", \"aren\", \"aren't\", \"around\", \"as\", \"at\", \"back\", \"been\", \"before\", \"beforehand\", \"behind\", \"being\", \"below\", \"beside\", \"besides\", \"between\", \"beyond\", \"both\", \"but\", \"by\", \"can\", \"cannot\", \"could\", \"couldn\", \"couldn't\", \"d\", \"didn\", \"didn't\", \"doesn\", \"doesn't\", \"don\", \"don't\", \"down\", \"due\", \"during\", \"either\", \"else\", \"elsewhere\", \"empty\", \"enough\", \"even\", \"ever\", \"everyone\", \"everything\", \"everywhere\", \"except\", \"first\", \"for\", \"former\", \"formerly\", \"from\", \"hadn\", \"hadn't\", \"hasn\", \"hasn't\", \"haven\", \"haven't\", \"he\", \"hence\", \"her\", \"here\", \"hereafter\", \"hereby\", \"herein\", \"hereupon\", \"hers\", \"herself\", \"him\", \"himself\", \"his\", \"how\", \"however\", \"hundred\", \"i\", \"if\", \"in\", \"indeed\", \"into\", \"is\", \"isn\", \"isn't\", \"it\", \"it's\", \"its\", \"itself\", \"just\", \"latter\", \"latterly\", \"least\", \"ll\", \"may\", \"me\", \"meanwhile\", \"mightn\", \"mightn't\", \"mine\", \"more\", \"moreover\", \"most\", \"mostly\", \"must\", \"mustn\", \"mustn't\", \"my\", \"myself\", \"namely\", \"needn\", \"needn't\", \"neither\", \"never\", \"nevertheless\", \"next\", \"no\", \"nobody\", \"none\", \"noone\", \"nor\", \"not\", \"nothing\", \"now\", \"nowhere\", \"o\", \"of\", \"off\", \"on\", \"once\", \"one\", \"only\", \"onto\", \"or\", \"other\", \"others\", \"otherwise\", \"our\", \"ours\", \"ourselves\", \"out\", \"over\", \"per\", \"please\", \"s\", \"same\", \"shan\", \"shan't\", \"she\", \"she's\", \"should've\", \"shouldn\", \"shouldn't\", \"somehow\", \"something\", \"sometime\", \"somewhere\", \"such\", \"t\", \"than\", \"that\", \"that'll\", \"the\", \"their\", \"theirs\", \"them\", \"themselves\", \"then\", \"thence\", \"there\", \"thereafter\", \"thereby\", \"therefore\", \"therein\", \"thereupon\", \"these\", \"they\", \"this\", \"those\", \"through\", \"throughout\", \"thru\", \"thus\", \"to\", \"too\", \"toward\", \"towards\", \"under\", \"unless\", \"until\", \"up\", \"upon\", \"used\", \"ve\", \"was\", \"wasn\", \"wasn't\", \"we\", \"were\", \"weren\", \"weren't\", \"what\", \"whatever\", \"when\", \"whence\", \"whenever\", \"where\", \"whereafter\", \"whereas\", \"whereby\", \"wherein\", \"whereupon\", \"wherever\", \"whether\", \"which\", \"while\", \"whither\", \"who\", \"whoever\", \"whole\", \"whom\", \"whose\", \"why\", \"with\", \"within\", \"without\", \"won\", \"won't\", \"would\", \"wouldn\", \"wouldn't\", \"y\", \"yet\", \"you\", \"you'd\", \"you'll\", \"you're\", \"you've\", \"your\", \"yours\", \"yourself\", \"yourselves\"]\n",
    "    )\n",
    "    # fmt: on\n",
    "    constraints = [RepeatModification(), StopwordModification(stopwords=stopwords)]\n",
    "    #\n",
    "    # During entailment, we should only edit the hypothesis - keep the premise\n",
    "    # the same.\n",
    "    #\n",
    "    input_column_modification = InputColumnModification(\n",
    "        [\"premise\", \"hypothesis\"], {\"premise\"}\n",
    "    )\n",
    "    constraints.append(input_column_modification)\n",
    "    # Minimum word embedding cosine similarity of 0.5.\n",
    "    # (The paper claims 0.7, but analysis of the released code and some empirical\n",
    "    # results show that it's 0.5.)\n",
    "    #\n",
    "    \n",
    "    constraints.append(WordEmbeddingDistance(min_cos_sim=0.5))\n",
    "    \n",
    "    #\n",
    "    # Only replace words with the same part of speech (or nouns with verbs)\n",
    "    #\n",
    "    \n",
    "    #constraints.append(PartOfSpeech(allow_verb_noun_swap=True))\n",
    "    \n",
    "    #\n",
    "    # Universal Sentence Encoder with a minimum angular similarity of Îµ = 0.5.\n",
    "    #\n",
    "    # In the TextFooler code, they forget to divide the angle between the two\n",
    "    # embeddings by pi. So if the original threshold was that 1 - sim >= 0.5, the\n",
    "    # new threshold is 1 - (0.5) / pi = 0.840845057\n",
    "    #\n",
    "    use_constraint = UniversalSentenceEncoder(\n",
    "        threshold=0.84,\n",
    "        metric=\"angular\",\n",
    "        compare_against_original=False,\n",
    "        window_size=15,\n",
    "        skip_text_shorter_than_window=True,\n",
    "    )\n",
    "    constraints.append(use_constraint)\n",
    "    #\n",
    "    # Goal is untargeted classification\n",
    "    #\n",
    "    goal_function = UntargetedClassification(model)\n",
    "    #\n",
    "    # Greedily swap words with \"Word Importance Ranking\".\n",
    "    #\n",
    "    search_method = GreedyWordSwapWIR(wir_method=\"delete\")\n",
    "\n",
    "    return Attack(goal_function, constraints, transformation, search_method)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "molecular-cherry",
   "metadata": {},
   "outputs": [],
   "source": [
    "#recipe = TextFoolerJin2019(\n",
    "#    goal_function=UntargetedClassification(model), \n",
    "#    search_method=GreedyWordSwapWIR(wir_method=\"delete\"),\n",
    "#    transformation=WordSwapEmbedding(max_candidates=50),\n",
    "#)\n",
    "#recipe.constraints = constraints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "sitting-pontiac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "result_df = pd.DataFrame({'attack_result': [], 'cross_val_batch': []})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "referenced-saint",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_names = [f'ru_train_crossval_{fold_id}' for fold_id in range(5)]\n",
    "model_names = [f'xlm_roberta_model_ru_crossval_{fold_id}' for fold_id in range(5)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "documentary-offset",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 LABELS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "- This IS expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Using /tmp/tfhub_modules to cache modules.\n",
      "\u001b[34;1mtextattack\u001b[0m: Unknown if model of class <class 'transformers.models.xlm_roberta.modeling_xlm_roberta.XLMRobertaForSequenceClassification'> compatible with goal function <class 'textattack.goal_functions.classification.untargeted_classification.UntargetedClassification'>.\n",
      "4it [00:15,  3.59s/it]"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "for batch_id, (dataset_name, model_name) in enumerate(zip(dataset_names, model_names)):\n",
    "    cur_dataset = __import__(dataset_name).dataset\n",
    "    cur_model = __import__(model_name).model\n",
    "    attacker = build_textfooler(cur_model)\n",
    "    for idx, result in tqdm(enumerate(attacker.attack_dataset(cur_dataset))):\n",
    "        result_df = result_df.append({\n",
    "            'attack_result': result.__str__(color_method='html'),\n",
    "            'cross_val_batch': batch_id\n",
    "        }, ignore_index=True)\n",
    "        #print(('x' * 20), f'Result {idx+1}', ('x' * 20))\n",
    "        #print(result.__str__(color_method='html'))\n",
    "        #print(len(str(result).split('\\n\\n')))\n",
    "    result_df.to_csv('new_ru_xlm_roberta_cross_val_result_30.csv')\n",
    "    del cur_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "quantitative-soviet",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(result_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stupid-composition",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "arctic-clarity",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_color_tags(text):\n",
    "    text = re.sub(r'<font color = [a-zA-Z]+>', ' ', text)\n",
    "    text = re.sub(r'</font>', ' ', text)\n",
    "    text = text.replace('  ', ' ')\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cardiac-narrative",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_targets = []\n",
    "\n",
    "for dataset_name in dataset_names:\n",
    "    dataset_labels = [id_to_label[pair[1]] for pair in __import__(dataset_name).dataset]\n",
    "    all_targets = all_targets + dataset_labels\n",
    "print(all_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "julian-recognition",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "ru_attacked_df = pd.DataFrame()\n",
    "attack_count = collections.defaultdict(int)\n",
    "word_count = collections.defaultdict(list)\n",
    "good_count = 0\n",
    "\n",
    "for raw_text, real_target in zip(result_df.attack_result, all_targets):\n",
    "    if len(raw_text.split('\\n\\n')) == 3:\n",
    "        header, old, new = raw_text.split('\\n\\n')\n",
    "        _, label_id1, label_id2 = re.split(r'<font color = [a-zA-Z]+>', header)\n",
    "        label1 = id_to_label[int(label_id1.split()[0])]\n",
    "        label2 = id_to_label[int(label_id2.split()[0])]\n",
    "        changed_words = len(tuple(re.finditer(r'<font color = [a-zA-Z]+>', old)))\n",
    "        word_count[(label1, label2)].append(changed_words)\n",
    "        attack_count[(label1, label2)] += 1\n",
    "        \n",
    "        \n",
    "        ru_attacked_df = ru_attacked_df.append(\n",
    "            {'old_text': remove_color_tags(old), \n",
    "             'text': remove_color_tags(new),\n",
    "             'changed_words_num': changed_words,\n",
    "             'old_model_target': label1,\n",
    "             'new_model_target': label2,\n",
    "             'target': real_target\n",
    "            },\n",
    "            ignore_index=True\n",
    "        )\n",
    "        good_count += 1\n",
    "print(good_count)\n",
    "\n",
    "clean_stat_df = pd.DataFrame()\n",
    "for (label1, label2) in attack_count.keys():\n",
    "    cur_attack_count = attack_count[(label1, label2)]\n",
    "    cur_word_count = word_count[(label1, label2)]\n",
    "    clean_stat_df = clean_stat_df.append(\n",
    "        {'model_label_old': label1, \n",
    "         'model_label_new': label2, \n",
    "         'mean_words': sum(cur_word_count) / cur_attack_count, \n",
    "         'median': np.median(cur_word_count)\n",
    "        }, \n",
    "        ignore_index=True\n",
    "    )\n",
    "clean_stat_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "equivalent-clear",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "718\n"
     ]
    }
   ],
   "source": [
    "print(good_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "laden-ceramic",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_stat_df.to_csv('/home/mlepekhin/data/new_ru_textfooler_stat_30.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sensitive-integrity",
   "metadata": {},
   "outputs": [],
   "source": [
    "ru_attacked_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "helpful-remedy",
   "metadata": {},
   "outputs": [],
   "source": [
    "ru_attacked_df.to_csv('/home/mlepekhin/data/new_ru_attacked_30.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wired-arbor",
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_color_tags('<font color = gray>ÐÐ¸Ð¶Ð½ÐµÐºÐ°Ð¼ÑÐºÐ½ÐµÑÑÐµÑÐ¸Ð¼')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "choice-baseline",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
